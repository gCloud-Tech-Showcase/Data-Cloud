# Getting Started

Deploy the Data-Cloud project infrastructure and run your first pipeline.

---

## Prerequisites

- **Google Cloud Project** with billing enabled
- **Terraform** >= 1.6.0 ([Download](https://www.terraform.io/downloads))
- **GitHub personal access token** with `repo` scope ([Create token](https://github.com/settings/tokens))
- **Python 3.9+** (optional, for review scraping)

---

## Step 1: Clone the Repository

```bash
git clone https://github.com/gCloud-Tech-Showcase/Data-Cloud.git
cd Data-Cloud
```

---

## Step 2: Configure Project Settings

Two files need your project ID:

### A. Terraform Configuration

```bash
cp infra/terraform.tfvars.example infra/terraform.tfvars
```

Edit `infra/terraform.tfvars`:

```hcl
project_id   = "your-project-id"           # Required: Your GCP project ID
github_token = "ghp_your_token_here"       # Required: GitHub personal access token

# Optional overrides (defaults are fine for most cases)
# region           = "us-central1"
# dataset_location = "US"
```

### B. Dataform Configuration

```bash
cp workflow_settings.yaml.example workflow_settings.yaml
```

Edit `workflow_settings.yaml`:

```yaml
defaultProject: your-project-id    # Must match terraform.tfvars
defaultLocation: US
defaultDataset: propensity_modeling
```

**Security note:** Both files are gitignored. Never commit them.

---

## Step 3: Deploy Infrastructure

```bash
cd infra
terraform init
terraform plan    # Preview changes
terraform apply   # Deploy (type 'yes' to confirm)
```

### What Gets Created

| Resource | Purpose |
|----------|---------|
| **BigQuery Datasets** | `sentiment_analysis`, `propensity_modeling`, `ga4_source`, `campaign_intelligence` |
| **GCS Bucket** | Storage for review JSON files |
| **BigQuery Connection** | Enables BigLake and Gemini access |
| **Dataform Repository** | Connected to GitHub |
| **Secret Manager** | Secure storage for GitHub token |
| **Service Account** | Dataform execution identity with required IAM roles |

Terraform automatically enables required APIs (BigQuery, Dataform, Vertex AI, etc.).

---

## Step 4: Create Dataform Workspace (Manual)

1. Open **Google Cloud Console** → **Dataform**
2. Click on the `data-cloud` repository
3. Click **Create Development Workspace**
4. Name it `main` (or your branch name)
5. Click **Create**

---

## Step 5: Run the Pipeline (Manual)

### Compile

1. In your Dataform workspace, click **Start Compilation**
2. Wait for compilation to complete (~10 seconds)

### Execute

1. Click **Start Execution**
2. Select **All actions** (or filter by tag: `sentiment_analysis`, `propensity_modeling`, `campaign_intelligence`)
3. Click **Execute**

**First run takes ~10-15 minutes** (model training). Subsequent runs are faster due to incremental processing.

---

## Step 6: Verify Deployment

In **BigQuery Console**, run:

```sql
-- Check sentiment analysis
SELECT sentiment, COUNT(*) AS count
FROM `sentiment_analysis.silver_review_sentiment`
GROUP BY sentiment;

-- Check propensity modeling
SELECT COUNT(*) AS training_rows
FROM `propensity_modeling.gold_training_features`;

-- Check model exists
SELECT * FROM ML.EVALUATE(MODEL `propensity_modeling.gold_user_retention_model`);
```

In **Vertex AI Console** → **Model Registry**, verify `gold_user_retention_model` is registered.

---

## Optional: Collect Fresh Reviews

The GCS bucket includes pre-scraped reviews (~500+). To scrape fresh data:

```bash
cd scripts
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python scrape_play_store_reviews.py
```

The scraper uses credentials auto-generated by Terraform (`.env` and `service-account-key.json`).

---

## Troubleshooting

| Issue | Solution |
|-------|----------|
| Dataform compilation fails with "undefined variable" | Check `workflow_settings.yaml` has correct `defaultProject` |
| `silver_review_sentiment` fails with "Connection not found" | Re-run `terraform apply` to recreate the Vertex AI connection |
| Model training fails with "Insufficient data" | Verify `gold_training_features` has 5K+ rows with balanced classes |
| Gemini API calls fail with "Permission denied" | Re-run `terraform apply` — IAM bindings may not have propagated |

---

## Cleanup

To remove all resources:

```bash
cd infra
terraform destroy
```

**Note:** GCS bucket is preserved by default. Delete manually in Cloud Console if needed.

---

## Next Steps

- **[Demo Guides](demos/README.md)** — Run the demonstrations
- **[Architecture](reference/architecture.md)** — Technical deep dive
